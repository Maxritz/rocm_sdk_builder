From db6a07afd44df332b9126d2987c1425aa21c4144 Mon Sep 17 00:00:00 2001
From: Jerry Mannil <65309407+jerrymannil@users.noreply.github.com>
Date: Thu, 21 Nov 2024 09:12:45 -0800
Subject: [PATCH 37/40] Enable vector size for 8 for half precision types in
 elementwise kernels (#1671) (#1738)

Enable *_load_dwordx4 ISA for BFloat16 and Half by using vector size of
8

Co-author: @akadutta
---
 aten/src/ATen/native/cuda/CUDALoops.cuh    |  5 ++
 aten/src/ATen/native/cuda/Dropout.cu       | 55 ++++++++++++++++++++--
 aten/src/ATen/native/cuda/MemoryAccess.cuh | 12 +++++
 3 files changed, 69 insertions(+), 3 deletions(-)

diff --git a/aten/src/ATen/native/cuda/CUDALoops.cuh b/aten/src/ATen/native/cuda/CUDALoops.cuh
index b8eb85fd4eb..94417bae449 100644
--- a/aten/src/ATen/native/cuda/CUDALoops.cuh
+++ b/aten/src/ATen/native/cuda/CUDALoops.cuh
@@ -116,6 +116,11 @@ static inline void launch_vectorized_kernel(
   int vec_size = memory::can_vectorize_up_to<func_t>(data);
 
   switch (vec_size) {
+    case 8:
+      vectorized_elementwise_kernel<8, func_t, array_t>
+          <<<grid, num_threads(), 0, stream>>>(N, f, data);
+      C10_CUDA_KERNEL_LAUNCH_CHECK();
+      break;
     case 4:
       vectorized_elementwise_kernel<4, func_t, array_t>
           <<<grid, num_threads(), 0, stream>>>(N, f, data);
diff --git a/aten/src/ATen/native/cuda/Dropout.cu b/aten/src/ATen/native/cuda/Dropout.cu
index 690051e6790..f32be4441e3 100644
--- a/aten/src/ATen/native/cuda/Dropout.cu
+++ b/aten/src/ATen/native/cuda/Dropout.cu
@@ -50,8 +50,13 @@ fused_dropout_kernel_vec(at::cuda::detail::TensorInfo<const scalar_t, IndexType>
                          at::cuda::detail::TensorInfo<mask_t, IndexType> c,
                          IndexType totalElements, accscalar_t p,
                          PhiloxCudaState philox_args) {
+#ifdef USE_ROCM
+  // make sure we don't break assumption that we can't have > 8 elements / thread
+  static_assert(VEC <= 8, "Value of VEC must be in [2, 4, 8]");
+#else
   // make sure we don't break assumption that we can't have > 4 elements / thread
   static_assert(VEC <= 4, "Value of VEC must be in [2, 4]");
+#endif
 
   using LoadT = memory::aligned_vector<scalar_t, VEC>;
   using MaskLoadT = memory::aligned_vector<mask_t, VEC>;
@@ -70,6 +75,9 @@ fused_dropout_kernel_vec(at::cuda::detail::TensorInfo<const scalar_t, IndexType>
   accscalar_t scale = 1.0 / p;
 
   float4 rand;
+#ifdef USE_ROCM
+  float4 rand1;
+#endif
 
   // Note: Vectorized loads means we'll stride each thread by an additional VEC factor, as we'll load VEC elements at a time
   for (IndexType linearIndex = idx * VEC;
@@ -83,7 +91,7 @@ fused_dropout_kernel_vec(at::cuda::detail::TensorInfo<const scalar_t, IndexType>
     //curand_uniform_double was pure evil anyway, not doing what it promises, and there's nothing for halfs, so generate float for everything
     // Note: need a new set of random values per 4 elements -- we'll handle VEC elements in this thread, so need ceil(VEC / 4)
     // sets of rand.
-    if ((VEC == 4) || (gridxvec_loop_state == 0)) {
+    if ((VEC >= 4) || (gridxvec_loop_state == 0)) {
       rand = curand_uniform4(&state);
     } else {
       // sets up the last two values we generated last iteration to be used this iteration.
@@ -92,12 +100,26 @@ fused_dropout_kernel_vec(at::cuda::detail::TensorInfo<const scalar_t, IndexType>
       gridxvec_loop_state ^= 1;
     }
 
+#ifdef USE_ROCM
+    if (VEC == 8) {
+      rand1 = curand_uniform4(&state);
+    }
+#endif
+
     rand.x = rand.x < p;
     rand.y = rand.y < p;
-    if (VEC == 4) {
+    if (VEC >= 4) {
       rand.z = rand.z < p;
       rand.w = rand.w < p;
     }
+#ifdef USE_ROCM
+    if (VEC == 8) {
+      rand1.x = rand1.x < p;
+      rand1.y = rand1.y < p;
+      rand1.z = rand1.z < p;
+      rand1.w = rand1.w < p;
+    }
+#endif
 
     // Note: We explicitly check for is_contiguous() before launching the vectorized kernel
     // and replace IndexToOffset call with linearIndex to allow vectorization of NHWC (or other)
@@ -110,10 +132,19 @@ fused_dropout_kernel_vec(at::cuda::detail::TensorInfo<const scalar_t, IndexType>
 
     // Perform the actual computation
     #pragma unroll
-    for (int ii = 0; ii < VEC; ii++) {
+    for (int ii = 0; ii < std::min(VEC, 4); ii++) {
       r[ii] = src[ii]*(&rand.x)[ii]*scale;
       mask[ii] = (mask_t)(&rand.x)[ii];
     }
+#ifdef USE_ROCM
+    if (VEC == 8) {
+      #pragma unroll
+      for (int ii = 0; ii < 4; ii++) {
+        r[4+ii] = src[4+ii]*(&rand1.x)[ii]*scale;
+        mask[4+ii] = (mask_t)(&rand1.x)[ii];
+      }
+    }
+#endif
     // Vectorized writes for both mask & result
     *(reinterpret_cast<LoadT*>(&b.data[linearIndex])) = *reinterpret_cast<LoadT*>(&r[0]);
     *(reinterpret_cast<MaskLoadT*>(&c.data[linearIndex])) = *reinterpret_cast<MaskLoadT*>(&mask[0]);
@@ -250,6 +281,22 @@ inline void launcher(
 
         if (vec_size > 1) {
           switch (vec_size) {
+            case 8:
+              fused_dropout_kernel_vec<
+                  scalar_t,
+                  accscalar_t,
+                  index_type,
+                  1,
+                  8>
+                  <<<grid, dim_block, 0, at::cuda::getCurrentCUDAStream()>>>(
+                      self_info,
+                      ret_info,
+                      mask_info,
+                      nelem,
+                      pa,
+                      rng_engine_inputs);
+              C10_CUDA_KERNEL_LAUNCH_CHECK();
+              break;
             case 4:
               fused_dropout_kernel_vec<
                   scalar_t,
@@ -282,6 +329,8 @@ inline void launcher(
                       rng_engine_inputs);
               C10_CUDA_KERNEL_LAUNCH_CHECK();
               break;
+	    default:
+              TORCH_INTERNAL_ASSERT(false, "Unexpected vectorization size");
           }
         } else {
           switch (self_info.dims) {
diff --git a/aten/src/ATen/native/cuda/MemoryAccess.cuh b/aten/src/ATen/native/cuda/MemoryAccess.cuh
index 0fdc813fd77..1662d58789a 100644
--- a/aten/src/ATen/native/cuda/MemoryAccess.cuh
+++ b/aten/src/ATen/native/cuda/MemoryAccess.cuh
@@ -350,11 +350,23 @@ inline C10_HOST_DEVICE int can_vectorize_up_to(const char *pointer) {
   uint64_t address = reinterpret_cast<uint64_t>(pointer);
   constexpr int vec2_alignment = std::alignment_of<aligned_vector<scalar_t, 2>>::value;
   constexpr int vec4_alignment = std::alignment_of<aligned_vector<scalar_t, 4>>::value;
+#if defined(USE_ROCM)
+  constexpr int vec8_alignment = std::alignment_of<aligned_vector<scalar_t, 8>>::value;
+  constexpr bool half_dtype = std::is_same_v<c10::BFloat16, scalar_t> || std::is_same_v<c10::Half, scalar_t>;
+  if (half_dtype && (address % vec8_alignment == 0)) {
+    return 8;
+  } else if (address % vec4_alignment == 0) {
+    return 4;
+  } else if (address % vec2_alignment == 0) {
+    return 2;
+  }
+#else
   if (address % vec4_alignment == 0) {
     return 4;
   } else if (address % vec2_alignment == 0) {
     return 2;
   }
+#endif
   return 1;
 }
 
-- 
2.43.0

